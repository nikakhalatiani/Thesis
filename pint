#!/usr/bin/env python3
"""
Extension to the pint CLI to support adaptive feedback loop testing.

This shows how to integrate adaptive testing into the existing CLI structure.
Add this to your pint file or create a new pint-adaptive command.
"""

import argparse
import sys
import os
import time
import json
from typing import Any

# Existing imports from pint
from core.properties import (
    create_symmetry_registry, create_algebraic_structure_registry,
    create_special_elements_registry, create_function_analysis_registry,
    create_composition_registry, create_comprehensive_registry,
    create_basic_registry, create_arithmetic_registry,
    create_logical_operations_registry, create_cryptographic_registry,
    create_data_structure_registry
)
from input.input_parser import InputParser
from config.property_inference_config import PropertyInferenceConfig
from core.function_under_test import FunctionUnderTest, ComparisonStrategy
from core.property_inference_engine import PropertyInferenceEngine
from config.grammar_config import GrammarConfig

# New imports for adaptive testing
from core.constraint_inference_engine import (
    RuleBasedModel, PythonScriptModel, LLMAPIModel, MockLLMModel
)
from core.adaptive_feedback_loop import AdaptiveFeedbackLoop, run_adaptive_property_test

# Add to existing REGISTRY_FACTORIES if integrating into pint
CONSTRAINT_MODEL_FACTORIES = {
    'rule': lambda args: RuleBasedModel(),
    'script': lambda args: PythonScriptModel(args.constraint_script),
    'llm': lambda args: LLMAPIModel(
        api_key=args.llm_api_key or os.environ.get('OPENAI_API_KEY', ''),
        model=args.llm_model
    ),
    'mock': lambda args: MockLLMModel(),
}


def add_adaptive_arguments(parser: argparse.ArgumentParser):
    """Add adaptive feedback loop arguments to parser."""

    adaptive_group = parser.add_argument_group('Adaptive Feedback Options')

    adaptive_group.add_argument(
        '--adaptive',
        action='store_true',
        help='Enable adaptive feedback loop for constraint inference'
    )

    adaptive_group.add_argument(
        '--constraint-model',
        choices=list(CONSTRAINT_MODEL_FACTORIES.keys()),
        default='rule',
        help='Constraint inference model to use (default: rule)'
    )

    adaptive_group.add_argument(
        '--max-feedback-iterations',
        type=int,
        default=5,
        help='Maximum iterations for adaptive feedback (default: 5)'
    )

    adaptive_group.add_argument(
        '--min-examples-per-iteration',
        type=int,
        default=50,
        help='Minimum examples per feedback iteration (default: 50)'
    )

    adaptive_group.add_argument(
        '--constraint-script',
        type=str,
        help='Path to Python script for constraint inference (when using script model)'
    )

    adaptive_group.add_argument(
        '--llm-api-key',
        type=str,
        help='API key for LLM constraint inference (or set OPENAI_API_KEY env var)'
    )

    adaptive_group.add_argument(
        '--llm-model',
        type=str,
        default='gpt-4',
        help='LLM model to use (default: gpt-4)'
    )

    adaptive_group.add_argument(
        '--save-constraints',
        type=str,
        help='Save inferred constraints to file'
    )

    return parser


def run_adaptive_analysis(config: PropertyInferenceConfig, args) -> dict:
    """Run adaptive analysis based on CLI arguments."""

    # Create constraint model
    try:
        constraint_model = CONSTRAINT_MODEL_FACTORIES[args.constraint_model](args)
    except Exception as e:
        print(f"Error creating constraint model: {e}", file=sys.stderr)
        sys.exit(1)

    # Create adaptive loop
    loop = AdaptiveFeedbackLoop(
        config,
        constraint_model=constraint_model,
        max_iterations=args.max_feedback_iterations,
        min_examples_per_iteration=args.min_examples_per_iteration,
        verbose=args.verbose
    )

    # Collect results for all property/function combinations
    all_results = {}
    all_constraints = {}

    # Get properties to test
    properties_to_test = config.properties_to_test or config.registry.get_all()

    from itertools import product
    from core.function_under_test import CombinedFunctionUnderTest

    for prop in properties_to_test:
        n = prop.num_functions

        for funcs in product(config.functions_under_test, repeat=n):
            combined = CombinedFunctionUnderTest(funcs, config.comparison_strategy)

            if not prop.is_applicable(combined):
                continue

            key = f"{combined.names()} - {prop.name}"

            if args.verbose:
                print(f"\n{'=' * 60}")
                print(f"Adaptive testing: {key}")
                print(f"{'=' * 60}")

            # Run adaptive test
            result = loop.run_adaptive_test(prop, combined)

            all_results[key] = result

            if result.final_constraints:
                func_key = combined.names()
                if func_key not in all_constraints:
                    all_constraints[func_key] = {}
                all_constraints[func_key][prop.name] = result.final_constraints

    # Save constraints if requested
    if args.save_constraints and all_constraints:
        try:
            with open(args.save_constraints, 'w') as f:
                json.dump(all_constraints, f, indent=2)
            if not args.quiet:
                print(f"\n‚úÖ Constraints saved to {args.save_constraints}")
        except Exception as e:
            print(f"Error saving constraints: {e}", file=sys.stderr)

    return all_results


def format_adaptive_results(results: dict, args) -> str:
    """Format adaptive results for output."""

    if args.json:
        # Convert to JSON-serializable format
        json_results = {}
        for key, result in results.items():
            json_results[key] = {
                'success': result.success,
                'iterations': len(result.iterations),
                'final_constraints': result.final_constraints,
                'total_time': result.total_time,
                'termination_reason': result.termination_reason
            }
        return json.dumps(json_results, indent=2)

    # Human-readable format
    output_lines = ["üìä Adaptive Feedback Loop Results", "=" * 60]

    # Summary statistics
    total = len(results)
    successful = sum(1 for r in results.values() if r.success)

    output_lines.extend([
        f"\nSummary:",
        f"  Total tests: {total}",
        f"  Successful: {successful} ({successful / total * 100:.1f}%)",
        ""
    ])

    # Detailed results
    for key, result in results.items():
        output_lines.extend([
            f"\n{key}:",
            f"  Status: {'‚úÖ PASS' if result.success else '‚ùå FAIL'}",
            f"  Iterations: {len(result.iterations)}",
            f"  Time: {result.total_time:.2f}s",
            f"  Reason: {result.termination_reason}"
        ])

        if result.final_constraints:
            output_lines.append("  Final constraints:")
            for constraint in result.final_constraints:
                output_lines.append(f"    - {constraint}")
        else:
            output_lines.append("  Final constraints: None")

        if args.verbose and result.iterations:
            output_lines.append("  Iteration details:")
            for it in result.iterations:
                output_lines.extend([
                    f"    Iteration {it.iteration}:",
                    f"      Tests: {it.test_result['stats']['total_count']}",
                    f"      Success rate: {it.test_result['stats']['success_count'] / it.test_result['stats']['total_count'] * 100:.1f}%",
                    f"      New constraints: {it.inferred_constraints or 'None'}"
                ])

    return "\n".join(output_lines)


def main_with_adaptive(args):
    """Extended main function with adaptive feedback support."""

    # Validate adaptive-specific arguments
    if args.adaptive:
        if args.constraint_model == 'script' and not args.constraint_script:
            print("Error: --constraint-script required when using script model", file=sys.stderr)
            sys.exit(1)

        if args.constraint_model == 'llm':
            api_key = args.llm_api_key or os.environ.get('OPENAI_API_KEY')
            if not api_key:
                print("Warning: No API key provided for LLM model, using mock instead", file=sys.stderr)
                args.constraint_model = 'mock'

    # Build configuration (same as original pint)
    registry_names = [name.strip() for name in args.registry.split(',')]
    # ... (rest of config setup as in original pint)

    # For demonstration, using simplified setup
    registry = create_basic_registry()
    config = (PropertyInferenceConfig(registry)
              .set_default_grammar(args.grammar)
              .set_default_parser(InputParser.for_numbers())
              .set_max_counterexamples(args.max_counterexamples)
              .set_example_count(args.examples))

    # Load and register functions (simplified for demo)
    # ... (load functions as in original pint)

    if args.adaptive:
        # Run adaptive analysis
        if not args.quiet:
            print("üîÑ Running adaptive feedback loop analysis...")

        start_time = time.perf_counter()
        results = run_adaptive_analysis(config, args)
        end_time = time.perf_counter()

        # Format and output results
        formatted_results = format_adaptive_results(results, args)

        if args.output:
            with open(args.output, 'w') as f:
                f.write(formatted_results)
            if not args.quiet:
                print(f"‚úÖ Results written to {args.output}")
        else:
            print(formatted_results)

        if args.timing:
            print(f"\n‚è±Ô∏è  Total execution time: {end_time - start_time:.4f} seconds")

    else:
        # Run standard analysis (original pint behavior)
        engine = PropertyInferenceEngine(config)
        results = engine.run()
        # ... (format and output as in original pint)


# Example of how to integrate into existing pint CLI
if __name__ == "__main__":
    # Create parser (would extend existing pint parser)
    parser = argparse.ArgumentParser(
        description="Property Inference with Adaptive Feedback"
    )

    # Add standard pint arguments
    parser.add_argument('-f', '--functions-file', default="input/user_input.py")
    parser.add_argument('-r', '--registry', default='basic')
    parser.add_argument('-e', '--examples', type=int, default=100)
    parser.add_argument('-g', '--grammar', default="grammars/test.fan")
    parser.add_argument('--max-counterexamples', type=int, default=3)
    parser.add_argument('-v', '--verbose', action='store_true')
    parser.add_argument('-q', '--quiet', action='store_true')
    parser.add_argument('--timing', action='store_true')
    parser.add_argument('--json', action='store_true')
    parser.add_argument('-o', '--output', type=str)

    # Add adaptive arguments
    parser = add_adaptive_arguments(parser)

    # Parse and run
    args = parser.parse_args()

    # Example usage message
    if len(sys.argv) == 1:
        print("Example usage:")
        print("  # Standard property inference")
        print("  python pint-adaptive.py -f my_functions.py")
        print()
        print("  # With adaptive feedback loop")
        print("  python pint-adaptive.py -f my_functions.py --adaptive")
        print()
        print("  # Using LLM for constraint inference")
        print("  python pint-adaptive.py -f my_functions.py --adaptive --constraint-model llm")
        print()
        print("  # Using custom script")
        print("  python pint-adaptive.py -f my_functions.py --adaptive \\")
        print("    --constraint-model script --constraint-script my_inference.py")
        print()
        print("  # Save inferred constraints")
        print("  python pint-adaptive.py -f my_functions.py --adaptive \\")
        print("    --save-constraints constraints.json")
        parser.print_help()
        sys.exit(0)

    main_with_adaptive(args)